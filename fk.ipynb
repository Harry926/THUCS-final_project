{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, json, jsonify\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import base64\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSON_AS_ASCII'] = False\n",
    "\n",
    "@app.route(\"/index\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('index.html')\n",
    "    if request.method == 'POST':\n",
    "        id = request.form.get('id')\n",
    "        data_tuple = find_relationship(id)\n",
    "        tweet_id_7_day = get_user_sentiment_7_day()\n",
    "        return render_template('test.html', data=users_name_url)\n",
    "\n",
    "@app.route(\"/detail\")\n",
    "def detail():\n",
    "    #ti7d = get_user_image()\n",
    "    uae = get_user_all_engagement(tweet_id_7_day)\n",
    "    ic = open(\"templates/icon.json\", \"r\",  encoding=\"utf-8\")\n",
    "    icon = json.load(ic)\n",
    "    ic.close()\n",
    "    img_stream = []\n",
    "    for i in range(15):\n",
    "        image_path1 = \"./image/\" + str(i) + \".png\"  \n",
    "        image_path2 = \"./image/\" + str(i) + \"c.png\"  #前端要這樣用 ex:<img src=\"data:;base64,{{ img1 }}\">\n",
    "        img_stream.append([return_img_stream(image_path1), return_img_stream(image_path2)])\n",
    "    return render_template('tttt.html', data=uae, icon=icon, img0=img_stream[0][0], img0c=img_stream[0][1], img1=img_stream[1][0], img1c=img_stream[1][1], img2=img_stream[2][0], img2c=img_stream[2][1], img3=img_stream[3][0], img3c=img_stream[3][1], img4=img_stream[4][0], img4c=img_stream[4][1], img5=img_stream[5][0], img5c=img_stream[5][1], img6=img_stream[6][0], img6c=img_stream[6][1], img7=img_stream[7][0], img7c=img_stream[7][1], img8=img_stream[8][0], img8c=img_stream[8][1], img9=img_stream[9][0], img9c=img_stream[9][1], img10=img_stream[10][0], img10c=img_stream[10][1], img11=img_stream[11][0], img11c=img_stream[11][1], img12=img_stream[12][0], img12c=img_stream[12][1], img13=img_stream[13][0], img13c=img_stream[13][1], img14=img_stream[14][0], img14c=img_stream[14][1])\n",
    "    #上面html檔 跟變數可以自己調整\n",
    "\n",
    "#config\n",
    "API_KEY = 'M16MB2eCRRvWCcfKDvr0qkxx5'\n",
    "API_SECRET = 'UEt06p8FCjWOdOa784wiXgp0k9XspBOuMMrL50KSvv9YYQoPeP'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAOtVZAEAAAAAiG%2BIch2KP0AYd6MvmzDeCrjOO9s%3DxtpORldSCmg3V9TXGTZjhevpJHmqS5MhNM4FDG23ZIllelYJbu'\n",
    "ACCESS_TOKEN = '1477449401719455745-j18PMElaCaxr2ZgPDx3xVMGuaULrdn'\n",
    "ACCESS_TOKEN_SECRET = '8JR9KBNpQAUkH5HxnvnZxHhPCZC97oaAXbf6ihQ0Fk88M'\n",
    "\n",
    "client = tweepy.Client(bearer_token = BEARER_TOKEN)\n",
    "\n",
    "users_name_url = {\"twitter\":[]}\n",
    "tweet_id_7_day = []\n",
    "\n",
    "model = torch.load('model3.pth',map_location ='cpu')\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def find_relationship(id):\n",
    "    ids = [id]\n",
    "    #get user_id\n",
    "    users = client.get_users(usernames=id, user_fields = ['profile_image_url'])\n",
    "    user_main = {}\n",
    "    for user in users.data:\n",
    "        USER_ID = user.id\n",
    "        USER_NAME = user.name\n",
    "        USER_URL = user.profile_image_url\n",
    "        USER_USERNAME = user.username\n",
    "\n",
    "    #get user tweet_id\n",
    "    tweet_id = []\n",
    "    t = client.get_users_tweets(id = USER_ID, max_results = 50, tweet_fields = ['conversation_id', 'in_reply_to_user_id'])\n",
    "    for i in t.data:\n",
    "        tweet_id.append(i.id)\n",
    "\n",
    "    #get user followers_id\n",
    "    followers_id = []  \n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_followers, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            followers_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get user following_id\n",
    "    following_id = []\n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_following, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            following_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get tweets reply_user\n",
    "    reply_user = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        query = 'conversation_id:' + str(tweet_id[k])\n",
    "        l = client.search_recent_tweets(query = query, max_results = 50, expansions = ['author_id'])\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                reply_user.append(i.author_id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets liking_users\n",
    "    liking_users = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_liking_users(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                liking_users.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets retweeters\n",
    "    retweeters = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_retweeters(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                retweeters.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "\n",
    "    #count relationship\n",
    "    total = {}\n",
    "    for i in liking_users:\n",
    "        if i in total.keys():\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in retweeters:\n",
    "        if(i in total.keys()):\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in reply_user:\n",
    "        if i in total.keys():\n",
    "            total[i] += 3\n",
    "        else:\n",
    "            total[i] = 3\n",
    "    \n",
    "    total[USER_ID] = 0\n",
    "    \n",
    "    result = sorted(total.items(), key=lambda x:x[1])[-14:]\n",
    "\n",
    "    result_rev = []\n",
    "    for i in range(len(result)-1, -1, -1):\n",
    "        result_rev.append(result[i])\n",
    "    \n",
    "    #top 10 user_data\n",
    "    related_id = {}\n",
    "    user_name = {}\n",
    "    for i in result:\n",
    "        u = client.get_users(ids = i[0], user_fields = ['profile_image_url'])\n",
    "        for k in u.data:\n",
    "            related_id[k.id] = []\n",
    "            user_name[k.id] = k.name\n",
    "            users_name_url[\"twitter\"].append({\"id\":str(k.id), \"name\":k.name, \"username\":k.username, \"url\":k.profile_image_url})\n",
    "    users_name_url[\"twitter\"].append({\"id\":str(USER_ID), \"name\":USER_NAME, \"username\":USER_USERNAME, \"url\":USER_URL})\n",
    "    for i in range(len(users_name_url['twitter'])):\n",
    "        users_name_url['twitter'][i]['url'] = users_name_url['twitter'][i]['url'].replace(\"normal.jpg\", \"400x400.jpg\")\n",
    "    return users_name_url\n",
    "\n",
    "\n",
    "\"\"\"def get_user_image():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    dtformat = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    times = datetime.now()\n",
    "    start_time = times - timedelta(days=7)\n",
    "    start_time, end_time = start_time.strftime(dtformat), time.strftime(dtformat)\n",
    "\n",
    "    tweets_id_7days = []\n",
    "\n",
    "    for k in range(len(users_name_url['twitter'])):\n",
    "        tmp = []\n",
    "        tweets_7days = []\n",
    "        idss = int(users_name_url['twitter'][k]['id'])\n",
    "        print(idss)\n",
    "        for response in tweepy.Paginator(client.get_users_tweets, id = idss, max_results=100, start_time = start_time, end_time = end_time, tweet_fields = [\"referenced_tweets\", \"created_at\"], limit=10):\n",
    "            for i in response:\n",
    "                tmp.append(i)\n",
    "        if tmp[0] != None:\n",
    "            for i in range(0, len(tmp), 4):\n",
    "                tmpp = []\n",
    "                for j in range(len(tmp[i])):\n",
    "                    tweets_7days.append({\"id\":users_name_url['twitter'][k]['id'], \"text\":tmp[i][j].text, \"time\":tmp[i][j].created_at})\n",
    "                    if(k == len(users_name_url['twitter'])-1):\n",
    "                        tweets_id_7days.append(tmp[i][j].id)\n",
    "                    \n",
    "            for i in range(len(tweets_7days)):\n",
    "                tweets_7days[i][\"text\"] = denoise_text(tweets_7days[i][\"text\"])\n",
    "                text = tweets_7days[i][\"text\"]\n",
    "                try:\n",
    "                    tweets_7days[i][\"text\"] = translator.translate(text, dest='zh-tw').text\n",
    "                except:\n",
    "                    print(end = \"\")\n",
    "\n",
    "            for j in range(len(tweets_7days)):\n",
    "                test_dataset = Dataset([tweets_7days[j][\"text\"]], [-1])\n",
    "                testloader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "                for i in testloader:\n",
    "                    tokens_tensors, segments_tensors, masks_tensors = i[:3]\n",
    "                    outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors)\n",
    "                    logits = outputs[0]\n",
    "                    labels = i[3]\n",
    "                    _, pred = torch.max(logits.data, 1)\n",
    "                    if pred[0] == 0:\n",
    "                        tweets_7days[j][\"sentiment\"] = 0\n",
    "                    elif pred[0] == 1:\n",
    "                        tweets_7days[j][\"sentiment\"] = 1\n",
    "                    elif pred[0] == 2:\n",
    "                        tweets_7days[j][\"sentiment\"] = 2\n",
    "                    elif pred[0] == 3:\n",
    "                        tweets_7days[j][\"sentiment\"] = 3\n",
    "                        \n",
    "            sentiment_0 = {}\n",
    "            sentiment_1 = {}\n",
    "            sentiment_2 = {}\n",
    "            sentiment_3 = {}\n",
    "            plt_day = []\n",
    "            plt_day_2 = []\n",
    "            \n",
    "            for i in range(8):\n",
    "                tt = times - timedelta(days=i)\n",
    "                sentiment_0[tt.day] = 0\n",
    "                sentiment_1[tt.day] = 0\n",
    "                sentiment_2[tt.day] = 0\n",
    "                sentiment_3[tt.day] = 0\n",
    "                plt_day.append(tt.day)\n",
    "                plt_day_2.append(str(tt.month) + \"/\" + str(tt.day))\n",
    "\n",
    "            for i in range(len(tweets_7days)):\n",
    "                if tweets_7days[i]['sentiment'] == 0:\n",
    "                        sentiment_0[tweets_7days[i]['time'].day] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 1:\n",
    "                        sentiment_1[tweets_7days[i]['time'].day] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 2:\n",
    "                        sentiment_2[tweets_7days[i]['time'].day] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 3:\n",
    "                        sentiment_3[tweets_7days[i]['time'].day] += 1\n",
    "\n",
    "            plt_sentiment_0 = []\n",
    "            plt_sentiment_1 = []\n",
    "            plt_sentiment_2 = []\n",
    "            plt_sentiment_3 = []\n",
    "            \n",
    "            plt_day.reverse()\n",
    "            plt_day_2.reverse()\n",
    "            \n",
    "            for i in plt_day:\n",
    "                plt_sentiment_0.append(sentiment_0[i])\n",
    "                plt_sentiment_1.append(sentiment_1[i])\n",
    "                plt_sentiment_2.append(sentiment_2[i])\n",
    "                plt_sentiment_3.append(sentiment_3[i])\n",
    "\n",
    "            plt.figure(figsize=(10,5),dpi=100,linewidth = 2)\n",
    "            tryy = [0,1,2,3,4,5,6,7]\n",
    "            plt.plot(tryy,plt_sentiment_0,'o-',color = 'r', label=\"happy\")\n",
    "            plt.plot(tryy,plt_sentiment_1,'o-',color = 'g', label=\"angry\")\n",
    "            plt.plot(tryy,plt_sentiment_2,'o-',color = 'b', label=\"sorrow\")\n",
    "            plt.plot(tryy,plt_sentiment_3,'o-',color = 'k', label=\"normal\")\n",
    "            plt.title(\"\", x=0.5, y=1.03)\n",
    "            \n",
    "            plt.xticks(tryy, plt_day_2, fontsize=20)\n",
    "            plt.yticks(fontsize=20)\n",
    "\n",
    "            plt.xlabel(\"date\", fontsize=15, labelpad = 15)\n",
    "            plt.ylabel(\"count\", fontsize=15, labelpad = 20)\n",
    "            plt.legend(loc = \"best\", fontsize=10)\n",
    "\n",
    "            plt.savefig('./image/'+ str(k) +'.png', dpi=300, bbox_inches='tight')\n",
    "            plt.clf()\n",
    "\n",
    "            df = pd.DataFrame([['happy', sum(plt_sentiment_0)], \n",
    "                            ['angry', sum(plt_sentiment_1)], \n",
    "                            ['sorrow', sum(plt_sentiment_2)], \n",
    "                            ['normal', sum(plt_sentiment_3)]], \n",
    "                            columns=['sentiment', 'count'])\n",
    "\n",
    "            fig = px.pie(df, values='count', names='sentiment', title='')\n",
    "            fig.update_traces(textposition='inside', textinfo='percent+label', insidetextorientation='radial')\n",
    "            fig.write_image('./image/'+str(k)+\"c.png\")\n",
    "    return tweets_id_7days \"\"\"\n",
    "\n",
    "\n",
    "def get_user_sentiment_7_day():\n",
    "    dtformat = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    times = datetime.now()\n",
    "    start_time = times - timedelta(days=7)\n",
    "    start_time, end_time = start_time.strftime(dtformat), time.strftime(dtformat)\n",
    "\n",
    "    tweets_id_7days = []\n",
    "\n",
    "    for k in range(len(users_name_url['twitter'])):\n",
    "        tmp = []\n",
    "        tweets_7days = []\n",
    "        idss = int(users_name_url['twitter'][k]['id'])\n",
    "        for response in tweepy.Paginator(client.get_users_tweets, id = idss, max_results=100, start_time = start_time, end_time = end_time, tweet_fields = [\"referenced_tweets\", \"created_at\"], limit=10):\n",
    "            for i in response:\n",
    "                tmp.append(i)\n",
    "        if tmp[0] != None:\n",
    "            for i in range(0, len(tmp), 4):\n",
    "                tmpp = []\n",
    "                for j in range(len(tmp[i])):\n",
    "                    tweets_7days.append({\"id\":users_name_url['twitter'][k]['id'], \"text\":tmp[i][j].text, \"time\":tmp[i][j].created_at})\n",
    "                    if(k == len(users_name_url['twitter'])-1):\n",
    "                        tweets_id_7days.append(tmp[i][j].id)\n",
    "                    \n",
    "            for i in range(len(tweets_7days)):\n",
    "                tweets_7days[i][\"text\"] = denoise_text(tweets_7days[i][\"text\"])\n",
    "                text = tweets_7days[i][\"text\"]\n",
    "                \"\"\"try:\n",
    "                    tweets_7days[i][\"text\"] = translator.translate(text, dest='zh-tw').text\n",
    "                except:\n",
    "                    print(end = \"\")\"\"\"\n",
    "\n",
    "            for j in range(len(tweets_7days)):\n",
    "                test_dataset = Dataset([tweets_7days[j][\"text\"]], [-1])\n",
    "                testloader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "                for i in testloader:\n",
    "                    tokens_tensors, segments_tensors, masks_tensors = i[:3]\n",
    "                    outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors)\n",
    "                    logits = outputs[0]\n",
    "                    labels = i[3]\n",
    "                    _, pred = torch.max(logits.data, 1)\n",
    "                    if pred[0] == 0:\n",
    "                        tweets_7days[j][\"sentiment\"] = 0\n",
    "                    elif pred[0] == 1:\n",
    "                        tweets_7days[j][\"sentiment\"] = 1\n",
    "                    elif pred[0] == 2:\n",
    "                        tweets_7days[j][\"sentiment\"] = 2\n",
    "                    elif pred[0] == 3:\n",
    "                        tweets_7days[j][\"sentiment\"] = 3\n",
    "            \n",
    "            sentiment = [0,0,0,0]\n",
    "\n",
    "            for i in range(len(tweets_7days)):\n",
    "                if tweets_7days[i]['sentiment'] == 0:\n",
    "                        sentiment[0] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 1:\n",
    "                        sentiment[1] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 2:\n",
    "                        sentiment[2] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 3:\n",
    "                        sentiment[3] += 1\n",
    "                        \n",
    "            m = max(sentiment)\n",
    "            for u in range(4):\n",
    "                if m == sentiment[u]:\n",
    "                    if u == 0:\n",
    "                        users_name_url['twitter'][k]['sentiment'] = \"happy\"\n",
    "                    if u == 1:\n",
    "                        users_name_url['twitter'][k]['sentiment'] = \"angry\"\n",
    "                    if u == 2:\n",
    "                        users_name_url['twitter'][k]['sentiment'] = \"sorrow\"\n",
    "                    if u == 3:\n",
    "                        users_name_url['twitter'][k]['sentiment'] = \"normal\"\n",
    "    return tweets_id_7days\n",
    "\n",
    "def get_user_all_engagement(tweets_id_7days):\n",
    "    reply_user = []\n",
    "    for k in range(len(tweets_id_7days)):\n",
    "        query = 'conversation_id:' + str(tweets_id_7days[k])\n",
    "        l = client.search_recent_tweets(query=query, max_results = 100, expansions = ['author_id'])\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                reply_user.append(i.author_id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "\n",
    "    liking_users = []\n",
    "    for k in range(len(tweets_id_7days)):\n",
    "        try:\n",
    "            l = client.get_liking_users(id = tweets_id_7days[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(360)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                liking_users.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "\n",
    "    retweeters = []\n",
    "    for k in range(len(tweets_id_7days)):\n",
    "        try:   \n",
    "            l = client.get_retweeters(id = tweets_id_7days[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(360)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                retweeters.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "\n",
    "\n",
    "    user_all_Engagement = {\"twitter\":[]}\n",
    "    a = []\n",
    "    for i in range(len(users_name_url['twitter'])):\n",
    "        a.append(int(users_name_url['twitter'][i]['id']))\n",
    "        user_all_Engagement['twitter'].append({\"id\":users_name_url['twitter'][i]['id']})\n",
    "\n",
    "    for i in liking_users:\n",
    "        if i in a:\n",
    "            for k in range(len(user_all_Engagement['twitter'])):\n",
    "                if int(user_all_Engagement['twitter'][k]['id']) == i:\n",
    "                    if \"liking\" not in user_all_Engagement['twitter'][k].keys():\n",
    "                        user_all_Engagement['twitter'][k]['liking'] = 0\n",
    "                    elif \"liking\" in user_all_Engagement['twitter'][k].keys():\n",
    "                        user_all_Engagement['twitter'][k]['liking'] += 1\n",
    "\n",
    "    for i in retweeters:\n",
    "        if i in a:\n",
    "            for k in range(len(user_all_Engagement['twitter'])):\n",
    "                if int(user_all_Engagement['twitter'][k]['id']) == i:\n",
    "                    if \"retweet\" not in user_all_Engagement['twitter'][k].keys():\n",
    "                        user_all_Engagement['twitter'][k]['retweet'] = 0\n",
    "                    elif \"retweet\" in user_all_Engagement['twitter'][k].keys():\n",
    "                        user_all_Engagement['twitter'][k]['retweet'] += 1\n",
    "    for i in reply_user:\n",
    "        if i in a:\n",
    "            for k in range(len(user_all_Engagement['twitter'])):\n",
    "                if int(user_all_Engagement['twitter'][k]['id']) == i:\n",
    "                    if \"reply\" not in user_all_Engagement['twitter'][k].keys():\n",
    "                        user_all_Engagement['twitter'][k]['reply'] = 0\n",
    "                    elif \"reply\" in user_all_Engagement['twitter'][k].keys():\n",
    "                        user_all_Engagement['twitter'][k]['reply'] += 1\n",
    "    for i in range(len(user_all_Engagement['twitter'])):\n",
    "        if \"liking\" not in user_all_Engagement['twitter'][i].keys():\n",
    "            user_all_Engagement['twitter'][i]['liking'] = 0\n",
    "        if \"retweet\" not in user_all_Engagement['twitter'][i].keys():\n",
    "            user_all_Engagement['twitter'][i]['retweet'] = 0\n",
    "        if \"reply\" not in user_all_Engagement['twitter'][i].keys():\n",
    "            user_all_Engagement['twitter'][i]['reply'] = 0\n",
    "\n",
    "    return user_all_Engagement\n",
    "\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#Removeing @...\n",
    "def remove_pattern(text):\n",
    "    return re.sub('@[\\w]*', '', text)\n",
    "\n",
    "#Removeing http\n",
    "def remove_http(text):\n",
    "    results = re.compile(r'[http|https]*://[a-zA-Z0-9._?/&=:]*', re.S) \n",
    "    return re.sub(results, '', text)\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = remove_pattern(text)\n",
    "    text = remove_http(text)\n",
    "    return text\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [data[i][0] for i in range(len(data))]\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    max_length=50,\n",
    "                    return_tensors='pt',\n",
    "                    return_length=True)\n",
    "    \n",
    "    input_ids = data['input_ids'] \n",
    "    attention_mask = data['attention_mask'] \n",
    "    token_type_ids = data['token_type_ids'] \n",
    "    labels = torch.LongTensor(labels) \n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tmp = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            tmp.append(pred[0])\n",
    "    return tmp\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#Removeing @...\n",
    "def remove_pattern(text):\n",
    "    return re.sub('@[\\w]*', '', text)\n",
    "\n",
    "#Removeing http\n",
    "def remove_http(text):\n",
    "    results = re.compile(r'[http|https]*://[a-zA-Z0-9._?/&=:]*', re.S) \n",
    "    return re.sub(results, '', text)\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = remove_pattern(text)\n",
    "    text = remove_http(text)\n",
    "    return text\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [data[i][0] for i in range(len(data))]\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    max_length=50,\n",
    "                    return_tensors='pt',\n",
    "                    return_length=True)\n",
    "    \n",
    "    input_ids = data['input_ids'] \n",
    "    attention_mask = data['attention_mask'] \n",
    "    token_type_ids = data['token_type_ids'] \n",
    "    labels = torch.LongTensor(labels) \n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tmp = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            tmp.append(pred[0])\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def return_img_stream(img_local_path):\n",
    "    img_stream = ''\n",
    "    with open(img_local_path, 'rb') as img_f:\n",
    "        img_stream = img_f.read()\n",
    "        img_stream = base64.b64encode(img_stream).decode()\n",
    "    return img_stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/Sep/2022 14:13:04] \"\u001b[37mGET /index HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Sep/2022 14:13:04] \"\u001b[37mGET /static/index_setting.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Sep/2022 14:13:05] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [19/Sep/2022 14:17:30] \"\u001b[37mPOST /index HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Sep/2022 14:17:30] \"\u001b[37mGET /static/concertric.css HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c86829d166ca73b9ab5589141ffcc20da4f172ad32a8d884566ed63425587a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
