{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport re\\n\\nfrom googletrans import Translator\\ntranslator = Translator()\\n\\n#Removeing @...\\ndef remove_pattern(text):\\n    return re.sub(\\'@[\\\\w]*\\', \\'\\', text)\\n\\n#Removeing http\\ndef remove_http(text):\\n    results = re.compile(r\\'[http|https]*://[a-zA-Z0-9._?/&=:]*\\', re.S) \\n    return re.sub(results, \\'\\', text)\\n\\n#Removing the noisy text\\ndef denoise_text(text):\\n    text = remove_pattern(text)\\n    text = remove_http(text)\\n    return text\\n\\n\\nimport numpy as np\\nimport torch \\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import BertTokenizer\\n\\nclass Dataset(Dataset):\\n    def __init__(self, x, y):\\n        self.x = x\\n        self.y = y\\n    \\n    def __len__(self):\\n        return len(self.x)\\n    \\n    def __getitem__(self, i):\\n        return self.x[i], self.y[i]\\n\\ntoken = BertTokenizer.from_pretrained(\\'bert-base-chinese\\')\\nmodel = torch.load(\\'model3.pth\\',map_location =\\'cpu\\')\\n\\ndef collate_fn(data):\\n    sents = [data[i][0] for i in range(len(data))]\\n    labels = [data[i][1] for i in range(len(data))]\\n    \\n    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\\n                    truncation=True, \\n                    padding=\\'max_length\\',\\n                    max_length=50,\\n                    return_tensors=\\'pt\\',\\n                    return_length=True)\\n    \\n    input_ids = data[\\'input_ids\\'] \\n    attention_mask = data[\\'attention_mask\\'] \\n    token_type_ids = data[\\'token_type_ids\\'] \\n    labels = torch.LongTensor(labels) \\n    \\n    return input_ids, attention_mask, token_type_ids, labels\\n\\ndef get_predictions(model, dataloader, compute_acc=False):\\n    predictions = None\\n    correct = 0\\n    total = 0\\n    tmp = []\\n    with torch.no_grad():\\n        for data in dataloader:\\n            \\n            tokens_tensors, segments_tensors, masks_tensors = data[:3]\\n            outputs = model(input_ids=tokens_tensors, \\n                            token_type_ids=segments_tensors, \\n                            attention_mask=masks_tensors)\\n            \\n            logits = outputs[0]\\n            _, pred = torch.max(logits.data, 1)\\n            tmp.append(pred[0])\\n    return tmp\\n\\ndef get_sentiment(related_id, user_name):\\n    for j in related_id.keys():\\n        t = client.get_users_tweets(id = j, max_results = 50, tweet_fields = [\"referenced_tweets\"])\\n        try:\\n            for i in t.data:\\n                if i.referenced_tweets != None:\\n                    if i.referenced_tweets[0].type == \"retweeted\" :\\n                        a = client.get_tweet(id = i.referenced_tweets[0].id)\\n                        related_id[j].append(a.data.text)\\n                    else:\\n                        related_id[j].append(i.text)\\n                else:\\n                    related_id[j].append(i.text)\\n        except:\\n            print(end = \"\")\\n    for i in related_id:\\n        for j in range(len(related_id[i])):\\n            related_id[i][j] = denoise_text(related_id[i][j])\\n            text = related_id[i][j]\\n            try:\\n                related_id[i][j] = translator.translate(text, dest=\\'zh-tw\\').text\\n            except:\\n                print(end = \"\")\\n\\n    Sentiment_result = {}\\n    for i in related_id:\\n        test_text = np.array(related_id[i], dtype=object)\\n        test_target = np.array([-1 for b in range(len(related_id[i]))], dtype=np.int64)\\n        test_dataset = Dataset(test_text, test_target)\\n        testloader = DataLoader(dataset=test_dataset,batch_size=1,collate_fn=collate_fn,shuffle=False,drop_last=False)\\n        Sentiment_result[i] = get_predictions(model, testloader, compute_acc=True)\\n    for i in Sentiment_result:\\n        print(\"id:\", user_name[i], \"\\n喜:\", Sentiment_result[i].count(0), \"怒:\", Sentiment_result[i].count(1), \"哀:\", Sentiment_result[i].count(2),\"中立:\", Sentiment_result[i].count(3))\\n        print()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, json, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSON_AS_ASCII'] = False\n",
    "\n",
    "@app.route(\"/index\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('index.html')\n",
    "    if request.method == 'POST':\n",
    "        id = request.form.get('id')\n",
    "        data, relat_id, us_na = find_relationship(id)\n",
    "        #sentiment = get_sentiment(relat_id, us_na)\n",
    "        return render_template('test.html', data=data)\n",
    "\n",
    "#config\n",
    "API_KEY = 'M16MB2eCRRvWCcfKDvr0qkxx5'\n",
    "API_SECRET = 'UEt06p8FCjWOdOa784wiXgp0k9XspBOuMMrL50KSvv9YYQoPeP'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAOtVZAEAAAAAiG%2BIch2KP0AYd6MvmzDeCrjOO9s%3DxtpORldSCmg3V9TXGTZjhevpJHmqS5MhNM4FDG23ZIllelYJbu'\n",
    "ACCESS_TOKEN = '1477449401719455745-j18PMElaCaxr2ZgPDx3xVMGuaULrdn'\n",
    "ACCESS_TOKEN_SECRET = '8JR9KBNpQAUkH5HxnvnZxHhPCZC97oaAXbf6ihQ0Fk88M'\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "client = tweepy.Client(bearer_token = BEARER_TOKEN)\n",
    "\n",
    "def find_relationship(id):\n",
    "    ids = [id]\n",
    "    #get user_id\n",
    "    users = client.get_users(usernames=id, user_fields = ['profile_image_url'])\n",
    "    user_main = {}\n",
    "    for user in users.data:\n",
    "        USER_ID = user.id\n",
    "        user_main_name = user.name\n",
    "        user_main_url = user.profile_image_url\n",
    "\n",
    "    #get user tweet_id\n",
    "    tweet_id = []\n",
    "    t = client.get_users_tweets(id = USER_ID, max_results = 50, tweet_fields = ['conversation_id', 'in_reply_to_user_id'])\n",
    "    for i in t.data:\n",
    "        tweet_id.append(i.id)\n",
    "\n",
    "    #get user followers_id\n",
    "    followers_id = []  \n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_followers, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            followers_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get user following_id\n",
    "    following_id = []\n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_following, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            following_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get tweets reply_user\n",
    "    reply_user = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        query = 'conversation_id:' + str(tweet_id[k])\n",
    "        l = client.search_recent_tweets(query = query, max_results = 50, expansions = ['author_id'])\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                reply_user.append(i.author_id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets liking_users\n",
    "    liking_users = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_liking_users(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                liking_users.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets retweeters\n",
    "    retweeters = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_retweeters(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                retweeters.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "\n",
    "    #count relationship\n",
    "    total = {}\n",
    "    for i in liking_users:\n",
    "        if i in total.keys():\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in retweeters:\n",
    "        if(i in total.keys()):\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in reply_user:\n",
    "        if i in total.keys():\n",
    "            total[i] += 3\n",
    "        else:\n",
    "            total[i] = 3\n",
    "    \n",
    "    total[USER_ID] = 0\n",
    "    \n",
    "    result = sorted(total.items(), key=lambda x:x[1])[-10:]\n",
    "\n",
    "    result_rev = []\n",
    "    for i in range(len(result)-1, 0, -1):\n",
    "        result_rev.append(result[i])\n",
    "    \n",
    "    #top 10 user_data\n",
    "    related_id = {}\n",
    "    user_name = {}\n",
    "    users_name_url = {}\n",
    "    for i in result_rev:\n",
    "        u = client.get_users(ids = i[0], user_fields = ['profile_image_url'])\n",
    "        for k in u.data:\n",
    "            related_id[k.id] = []\n",
    "            user_name[k.id] = k.name\n",
    "            users_name_url[str(k.id)] = {\"name\":k.name, \"url\":k.profile_image_url}\n",
    "    users_name_url[str(USER_ID)] = {\"name\":user_main_name, \"url\":user_main_url}\n",
    "    for i in users_name_url:\n",
    "        users_name_url[i]['url'] = users_name_url[i]['url'].replace(\"normal.jpg\", \"400x400.jpg\")\n",
    "    for i in users_name_url:\n",
    "        print(users_name_url[i]['name'])\n",
    "    return users_name_url, related_id, user_name\n",
    "    \n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#Removeing @...\n",
    "def remove_pattern(text):\n",
    "    return re.sub('@[\\w]*', '', text)\n",
    "\n",
    "#Removeing http\n",
    "def remove_http(text):\n",
    "    results = re.compile(r'[http|https]*://[a-zA-Z0-9._?/&=:]*', re.S) \n",
    "    return re.sub(results, '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = remove_pattern(text)\n",
    "    text = remove_http(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = torch.load('model3.pth',map_location ='cpu')\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [data[i][0] for i in range(len(data))]\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    max_length=50,\n",
    "                    return_tensors='pt',\n",
    "                    return_length=True)\n",
    "    \n",
    "    input_ids = data['input_ids'] \n",
    "    attention_mask = data['attention_mask'] \n",
    "    token_type_ids = data['token_type_ids'] \n",
    "    labels = torch.LongTensor(labels) \n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tmp = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            tmp.append(pred[0])\n",
    "    return tmp\n",
    "\n",
    "def get_sentiment(related_id, user_name):\n",
    "    for j in related_id.keys():\n",
    "        t = client.get_users_tweets(id = j, max_results = 50, tweet_fields = [\"referenced_tweets\"])\n",
    "        try:\n",
    "            for i in t.data:\n",
    "                if i.referenced_tweets != None:\n",
    "                    if i.referenced_tweets[0].type == \"retweeted\" :\n",
    "                        a = client.get_tweet(id = i.referenced_tweets[0].id)\n",
    "                        related_id[j].append(a.data.text)\n",
    "                    else:\n",
    "                        related_id[j].append(i.text)\n",
    "                else:\n",
    "                    related_id[j].append(i.text)\n",
    "        except:\n",
    "            print(end = \"\")\n",
    "    for i in related_id:\n",
    "        for j in range(len(related_id[i])):\n",
    "            related_id[i][j] = denoise_text(related_id[i][j])\n",
    "            text = related_id[i][j]\n",
    "            try:\n",
    "                related_id[i][j] = translator.translate(text, dest='zh-tw').text\n",
    "            except:\n",
    "                print(end = \"\")\n",
    "\n",
    "    Sentiment_result = {}\n",
    "    for i in related_id:\n",
    "        test_text = np.array(related_id[i], dtype=object)\n",
    "        test_target = np.array([-1 for b in range(len(related_id[i]))], dtype=np.int64)\n",
    "        test_dataset = Dataset(test_text, test_target)\n",
    "        testloader = DataLoader(dataset=test_dataset,batch_size=1,collate_fn=collate_fn,shuffle=False,drop_last=False)\n",
    "        Sentiment_result[i] = get_predictions(model, testloader, compute_acc=True)\n",
    "    for i in Sentiment_result:\n",
    "        print(\"id:\", user_name[i], \"\\n喜:\", Sentiment_result[i].count(0), \"怒:\", Sentiment_result[i].count(1), \"哀:\", Sentiment_result[i].count(2),\"中立:\", Sentiment_result[i].count(3))\n",
    "        print()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [12/Jul/2022 17:44:39] \"\u001b[37mGET /index HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c86829d166ca73b9ab5589141ffcc20da4f172ad32a8d884566ed63425587a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
