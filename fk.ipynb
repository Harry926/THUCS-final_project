{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-336b53a638e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model3.pth'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-chinese'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_relationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1730\u001b[0m                         \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m                         \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m                         \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1733\u001b[0m                     )\n\u001b[0;32m   1734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1927\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m         )\n\u001b[0;32m   1931\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   2122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2124\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2125\u001b[0m             \u001b[0m_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m             \u001b[0metag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Linked-Etag\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ETag\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'head'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    527\u001b[0m         }\n\u001b[0;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m                 )\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    670\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mssl_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         )\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password)\u001b[0m\n\u001b[0;32m    368\u001b[0m     ) or IS_SECURETRANSPORT:\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         warnings.warn(\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         )\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m    868\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\MEA\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, json, jsonify\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import base64\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSON_AS_ASCII'] = False\n",
    "\n",
    "@app.route(\"/index\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'GET':\n",
    "        return render_template('index.html')\n",
    "    if request.method == 'POST':\n",
    "        id = request.form.get('id')\n",
    "        data_tuple = find_relationship(id)\n",
    "        return render_template('test.html', data=users_name_url)\n",
    "\n",
    "@app.route(\"/detail\")\n",
    "def detail():\n",
    "    get_user_image()\n",
    "    ic = open(\"templates/icon.json\", \"r\",  encoding=\"utf-8\")\n",
    "    icon = json.load(ic)\n",
    "    ic.close()\n",
    "    img_stream = []\n",
    "    for i in range(15):\n",
    "        image_path1 = \"C:\\\\Users\\\\MEA\\\\flask\\\\image\\\\\" + str(i) + \".png\"   #要改成你的圖片位置\n",
    "        image_path2 = \"C:\\\\Users\\\\MEA\\\\flask\\\\image\\\\\" + str(i) + \"c.png\"  #要改成你的圖片位置 前端要這樣用 ex:<img src=\"data:;base64,{{ img1 }}\">\n",
    "        img_stream.append([return_img_stream(image_path1), return_img_stream(image_path2)])\n",
    "    return render_template('testpage3.html', icon=icon, img0=img_stream[0][0], img0c=img_stream[0][1], img1=img_stream[1][0], img1c=img_stream[1][1], img2=img_stream[2][0], img2c=img_stream[2][1], img3=img_stream[3][0], img3c=img_stream[3][1], img4=img_stream[4][0], img4c=img_stream[4][1], img5=img_stream[5][0], img5c=img_stream[5][1], img6=img_stream[6][0], img6c=img_stream[6][1], img7=img_stream[7][0], img7c=img_stream[7][1], img8=img_stream[8][0], img8c=img_stream[8][1], img9=img_stream[9][0], img9c=img_stream[9][1], img10=img_stream[10][0], img10c=img_stream[10][1], img11=img_stream[11][0], img11c=img_stream[11][1], img12=img_stream[12][0], img12c=img_stream[12][1], img13=img_stream[13][0], img13c=img_stream[13][1], img14=img_stream[14][0], img14c=img_stream[14][1])\n",
    "\n",
    "#config\n",
    "API_KEY = 'M16MB2eCRRvWCcfKDvr0qkxx5'\n",
    "API_SECRET = 'UEt06p8FCjWOdOa784wiXgp0k9XspBOuMMrL50KSvv9YYQoPeP'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAOtVZAEAAAAAiG%2BIch2KP0AYd6MvmzDeCrjOO9s%3DxtpORldSCmg3V9TXGTZjhevpJHmqS5MhNM4FDG23ZIllelYJbu'\n",
    "ACCESS_TOKEN = '1477449401719455745-j18PMElaCaxr2ZgPDx3xVMGuaULrdn'\n",
    "ACCESS_TOKEN_SECRET = '8JR9KBNpQAUkH5HxnvnZxHhPCZC97oaAXbf6ihQ0Fk88M'\n",
    "\n",
    "\n",
    "client = tweepy.Client(bearer_token = BEARER_TOKEN)\n",
    "\n",
    "users_name_url = {\"twitter\":[]}\n",
    "\n",
    "model = torch.load('model3.pth',map_location ='cpu')\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def find_relationship(id):\n",
    "    ids = [id]\n",
    "    #get user_id\n",
    "    users = client.get_users(usernames=id, user_fields = ['profile_image_url'])\n",
    "    user_main = {}\n",
    "    for user in users.data:\n",
    "        USER_ID = user.id\n",
    "        USER_NAME = user.name\n",
    "        USER_URL = user.profile_image_url\n",
    "        USER_USERNAME = user.username\n",
    "\n",
    "    #get user tweet_id\n",
    "    tweet_id = []\n",
    "    t = client.get_users_tweets(id = USER_ID, max_results = 50, tweet_fields = ['conversation_id', 'in_reply_to_user_id'])\n",
    "    for i in t.data:\n",
    "        tweet_id.append(i.id)\n",
    "\n",
    "    #get user followers_id\n",
    "    followers_id = []  \n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_followers, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            followers_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get user following_id\n",
    "    following_id = []\n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_following, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            following_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get tweets reply_user\n",
    "    reply_user = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        query = 'conversation_id:' + str(tweet_id[k])\n",
    "        l = client.search_recent_tweets(query = query, max_results = 50, expansions = ['author_id'])\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                reply_user.append(i.author_id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets liking_users\n",
    "    liking_users = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_liking_users(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                liking_users.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets retweeters\n",
    "    retweeters = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_retweeters(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                retweeters.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "\n",
    "    #count relationship\n",
    "    total = {}\n",
    "    for i in liking_users:\n",
    "        if i in total.keys():\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in retweeters:\n",
    "        if(i in total.keys()):\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in reply_user:\n",
    "        if i in total.keys():\n",
    "            total[i] += 3\n",
    "        else:\n",
    "            total[i] = 3\n",
    "    \n",
    "    total[USER_ID] = 0\n",
    "    \n",
    "    result = sorted(total.items(), key=lambda x:x[1])[-14:]\n",
    "\n",
    "    result_rev = []\n",
    "    for i in range(len(result)-1, -1, -1):\n",
    "        result_rev.append(result[i])\n",
    "    \n",
    "    #top 10 user_data\n",
    "    related_id = {}\n",
    "    user_name = {}\n",
    "    for i in result:\n",
    "        u = client.get_users(ids = i[0], user_fields = ['profile_image_url'])\n",
    "        for k in u.data:\n",
    "            related_id[k.id] = []\n",
    "            user_name[k.id] = k.name\n",
    "            users_name_url[\"twitter\"].append({\"id\":str(k.id), \"name\":k.name, \"username\":k.username, \"url\":k.profile_image_url})\n",
    "    users_name_url[\"twitter\"].append({\"id\":str(USER_ID), \"name\":USER_NAME, \"username\":USER_USERNAME, \"url\":USER_URL})\n",
    "    for i in range(len(users_name_url['twitter'])):\n",
    "        users_name_url['twitter'][i]['url'] = users_name_url['twitter'][i]['url'].replace(\"normal.jpg\", \"400x400.jpg\")\n",
    "    return users_name_url\n",
    "\n",
    "#config\n",
    "API_KEY = 'M16MB2eCRRvWCcfKDvr0qkxx5'\n",
    "API_SECRET = 'UEt06p8FCjWOdOa784wiXgp0k9XspBOuMMrL50KSvv9YYQoPeP'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAOtVZAEAAAAAiG%2BIch2KP0AYd6MvmzDeCrjOO9s%3DxtpORldSCmg3V9TXGTZjhevpJHmqS5MhNM4FDG23ZIllelYJbu'\n",
    "ACCESS_TOKEN = '1477449401719455745-j18PMElaCaxr2ZgPDx3xVMGuaULrdn'\n",
    "ACCESS_TOKEN_SECRET = '8JR9KBNpQAUkH5HxnvnZxHhPCZC97oaAXbf6ihQ0Fk88M'\n",
    "\n",
    "client = tweepy.Client(bearer_token = BEARER_TOKEN)\n",
    "\n",
    "users_name_url = {\"twitter\":[]}\n",
    "\n",
    "model = torch.load('model3.pth',map_location ='cpu')\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def find_relationship(id):\n",
    "    ids = [id]\n",
    "    #get user_id\n",
    "    users = client.get_users(usernames=id, user_fields = ['profile_image_url'])\n",
    "    user_main = {}\n",
    "    for user in users.data:\n",
    "        USER_ID = user.id\n",
    "        USER_NAME = user.name\n",
    "        USER_URL = user.profile_image_url\n",
    "        USER_USERNAME = user.username\n",
    "\n",
    "    #get user tweet_id\n",
    "    tweet_id = []\n",
    "    t = client.get_users_tweets(id = USER_ID, max_results = 50, tweet_fields = ['conversation_id', 'in_reply_to_user_id'])\n",
    "    for i in t.data:\n",
    "        tweet_id.append(i.id)\n",
    "\n",
    "    #get user followers_id\n",
    "    followers_id = []  \n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_followers, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            followers_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get user following_id\n",
    "    following_id = []\n",
    "    tmp = []\n",
    "    for response in tweepy.Paginator(client.get_users_following, USER_ID, max_results=1000, limit=2):\n",
    "        for i in response:\n",
    "            tmp.append(i)\n",
    "    for i in range(0, len(tmp), 4):\n",
    "        for j in range(len(tmp[i])):\n",
    "            following_id.append(tmp[i][j].id)\n",
    "\n",
    "    #get tweets reply_user\n",
    "    reply_user = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        query = 'conversation_id:' + str(tweet_id[k])\n",
    "        l = client.search_recent_tweets(query = query, max_results = 50, expansions = ['author_id'])\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                reply_user.append(i.author_id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets liking_users\n",
    "    liking_users = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_liking_users(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                liking_users.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "    #get tweets retweeters\n",
    "    retweeters = []\n",
    "    for k in range(len(tweet_id)):\n",
    "        try:\n",
    "            l = client.get_retweeters(id = tweet_id[k])\n",
    "        except:\n",
    "            print(\"sleep\")\n",
    "            time.sleep(60)\n",
    "        try:\n",
    "            for i in l.data:\n",
    "                retweeters.append(i.id)\n",
    "        except:\n",
    "            print(\"\", end = \"\")\n",
    "    \n",
    "\n",
    "    #count relationship\n",
    "    total = {}\n",
    "    for i in liking_users:\n",
    "        if i in total.keys():\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in retweeters:\n",
    "        if(i in total.keys()):\n",
    "            total[i] += 2\n",
    "        else:\n",
    "            total[i] = 2\n",
    "\n",
    "    for i in reply_user:\n",
    "        if i in total.keys():\n",
    "            total[i] += 3\n",
    "        else:\n",
    "            total[i] = 3\n",
    "    \n",
    "    total[USER_ID] = 0\n",
    "    \n",
    "    result = sorted(total.items(), key=lambda x:x[1])[-14:]\n",
    "\n",
    "    result_rev = []\n",
    "    for i in range(len(result)-1, -1, -1):\n",
    "        result_rev.append(result[i])\n",
    "    \n",
    "    #top 10 user_data\n",
    "    related_id = {}\n",
    "    user_name = {}\n",
    "    for i in result:\n",
    "        u = client.get_users(ids = i[0], user_fields = ['profile_image_url'])\n",
    "        for k in u.data:\n",
    "            related_id[k.id] = []\n",
    "            user_name[k.id] = k.name\n",
    "            users_name_url[\"twitter\"].append({\"id\":str(k.id), \"name\":k.name, \"username\":k.username, \"url\":k.profile_image_url})\n",
    "    users_name_url[\"twitter\"].append({\"id\":str(USER_ID), \"name\":USER_NAME, \"username\":USER_USERNAME, \"url\":USER_URL})\n",
    "    for i in range(len(users_name_url['twitter'])):\n",
    "        users_name_url['twitter'][i]['url'] = users_name_url['twitter'][i]['url'].replace(\"normal.jpg\", \"400x400.jpg\")\n",
    "    return users_name_url\n",
    "\n",
    "\n",
    "def get_user_image():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    dtformat = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    times = datetime.now()\n",
    "    start_time = times - timedelta(days=7)\n",
    "    start_time, end_time = start_time.strftime(dtformat), time.strftime(dtformat)\n",
    "\n",
    "    tweets_id_7days = []\n",
    "\n",
    "    for k in range(len(users_name_url['twitter'])):\n",
    "        tmp = []\n",
    "        tweets_7days = []\n",
    "        idss = int(users_name_url['twitter'][k]['id'])\n",
    "        print(idss)\n",
    "        for response in tweepy.Paginator(client.get_users_tweets, id = idss, max_results=100, start_time = start_time, end_time = end_time, tweet_fields = [\"referenced_tweets\", \"created_at\"], limit=10):\n",
    "            for i in response:\n",
    "                tmp.append(i)\n",
    "        if tmp[0] != None:\n",
    "            for i in range(0, len(tmp), 4):\n",
    "                tmpp = []\n",
    "                for j in range(len(tmp[i])):\n",
    "                    tweets_7days.append({\"id\":users_name_url['twitter'][k]['id'], \"text\":tmp[i][j].text, \"time\":tmp[i][j].created_at})\n",
    "                    if(k == len(users_name_url['twitter'])-1):\n",
    "                        tweets_id_7days.append(tmp[i][j].id)\n",
    "                    \n",
    "            for i in range(len(tweets_7days)):\n",
    "                tweets_7days[i][\"text\"] = denoise_text(tweets_7days[i][\"text\"])\n",
    "                text = tweets_7days[i][\"text\"]\n",
    "                \"\"\"try:\n",
    "                    tweets_7days[i][\"text\"] = translator.translate(text, dest='zh-tw').text\n",
    "                except:\n",
    "                    print(end = \"\")\"\"\"\n",
    "\n",
    "            for j in range(len(tweets_7days)):\n",
    "                test_dataset = Dataset([tweets_7days[j][\"text\"]], [-1])\n",
    "                testloader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "                for i in testloader:\n",
    "                    tokens_tensors, segments_tensors, masks_tensors = i[:3]\n",
    "                    outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors)\n",
    "                    logits = outputs[0]\n",
    "                    labels = i[3]\n",
    "                    _, pred = torch.max(logits.data, 1)\n",
    "                    if pred[0] == 0:\n",
    "                        tweets_7days[j][\"sentiment\"] = 0\n",
    "                    elif pred[0] == 1:\n",
    "                        tweets_7days[j][\"sentiment\"] = 1\n",
    "                    elif pred[0] == 2:\n",
    "                        tweets_7days[j][\"sentiment\"] = 2\n",
    "                    elif pred[0] == 3:\n",
    "                        tweets_7days[j][\"sentiment\"] = 3\n",
    "                        \n",
    "            sentiment_0 = {}\n",
    "            sentiment_1 = {}\n",
    "            sentiment_2 = {}\n",
    "            sentiment_3 = {}\n",
    "            plt_day = []\n",
    "            plt_day_2 = []\n",
    "            \n",
    "            for i in range(8):\n",
    "                tt = times - timedelta(days=i)\n",
    "                sentiment_0[tt.day] = 0\n",
    "                sentiment_1[tt.day] = 0\n",
    "                sentiment_2[tt.day] = 0\n",
    "                sentiment_3[tt.day] = 0\n",
    "                plt_day.append(tt.day)\n",
    "                plt_day_2.append(str(tt.month) + \"/\" + str(tt.day))\n",
    "\n",
    "            for i in range(len(tweets_7days)):\n",
    "                if tweets_7days[i]['sentiment'] == 0:\n",
    "                        sentiment_0[tweets_7days[i]['time'].day] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 1:\n",
    "                        sentiment_1[tweets_7days[i]['time'].day] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 2:\n",
    "                        sentiment_2[tweets_7days[i]['time'].day] += 1\n",
    "                elif tweets_7days[i]['sentiment'] == 3:\n",
    "                        sentiment_3[tweets_7days[i]['time'].day] += 1\n",
    "\n",
    "            plt_sentiment_0 = []\n",
    "            plt_sentiment_1 = []\n",
    "            plt_sentiment_2 = []\n",
    "            plt_sentiment_3 = []\n",
    "            \n",
    "            plt_day.reverse()\n",
    "            plt_day_2.reverse()\n",
    "            \n",
    "            for i in plt_day:\n",
    "                plt_sentiment_0.append(sentiment_0[i])\n",
    "                plt_sentiment_1.append(sentiment_1[i])\n",
    "                plt_sentiment_2.append(sentiment_2[i])\n",
    "                plt_sentiment_3.append(sentiment_3[i])\n",
    "\n",
    "            plt.figure(figsize=(10,5),dpi=100,linewidth = 2)\n",
    "            tryy = [0,1,2,3,4,5,6,7]\n",
    "            plt.plot(tryy,plt_sentiment_0,'o-',color = 'r', label=\"happy\")\n",
    "            plt.plot(tryy,plt_sentiment_1,'o-',color = 'g', label=\"angry\")\n",
    "            plt.plot(tryy,plt_sentiment_2,'o-',color = 'b', label=\"sorrow\")\n",
    "            plt.plot(tryy,plt_sentiment_3,'o-',color = 'k', label=\"normal\")\n",
    "            plt.title(\"\", x=0.5, y=1.03)\n",
    "            \n",
    "            plt.xticks(tryy, plt_day_2, fontsize=20)\n",
    "            plt.yticks(fontsize=20)\n",
    "\n",
    "            plt.xlabel(\"date\", fontsize=15, labelpad = 15)\n",
    "            plt.ylabel(\"count\", fontsize=15, labelpad = 20)\n",
    "            plt.legend(loc = \"best\", fontsize=10)\n",
    "\n",
    "            plt.savefig('./image/'+ str(k) +'.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "            df = pd.DataFrame([['happy', sum(plt_sentiment_0)], \n",
    "                            ['angry', sum(plt_sentiment_1)], \n",
    "                            ['sorrow', sum(plt_sentiment_2)], \n",
    "                            ['normal', sum(plt_sentiment_3)]], \n",
    "                            columns=['sentiment', 'count'])\n",
    "\n",
    "            fig = px.pie(df, values='count', names='sentiment', title='')\n",
    "            fig.update_traces(textposition='inside', textinfo='percent+label', insidetextorientation='radial')\n",
    "            fig.write_image('./image/'+str(k)+\"c.png\")\n",
    "            fig.show()\n",
    "\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#Removeing @...\n",
    "def remove_pattern(text):\n",
    "    return re.sub('@[\\w]*', '', text)\n",
    "\n",
    "#Removeing http\n",
    "def remove_http(text):\n",
    "    results = re.compile(r'[http|https]*://[a-zA-Z0-9._?/&=:]*', re.S) \n",
    "    return re.sub(results, '', text)\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = remove_pattern(text)\n",
    "    text = remove_http(text)\n",
    "    return text\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [data[i][0] for i in range(len(data))]\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    max_length=50,\n",
    "                    return_tensors='pt',\n",
    "                    return_length=True)\n",
    "    \n",
    "    input_ids = data['input_ids'] \n",
    "    attention_mask = data['attention_mask'] \n",
    "    token_type_ids = data['token_type_ids'] \n",
    "    labels = torch.LongTensor(labels) \n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tmp = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            tmp.append(pred[0])\n",
    "    return tmp\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#Removeing @...\n",
    "def remove_pattern(text):\n",
    "    return re.sub('@[\\w]*', '', text)\n",
    "\n",
    "#Removeing http\n",
    "def remove_http(text):\n",
    "    results = re.compile(r'[http|https]*://[a-zA-Z0-9._?/&=:]*', re.S) \n",
    "    return re.sub(results, '', text)\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = remove_pattern(text)\n",
    "    text = remove_http(text)\n",
    "    return text\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "def collate_fn(data):\n",
    "    sents = [data[i][0] for i in range(len(data))]\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    \n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                    truncation=True, \n",
    "                    padding='max_length',\n",
    "                    max_length=50,\n",
    "                    return_tensors='pt',\n",
    "                    return_length=True)\n",
    "    \n",
    "    input_ids = data['input_ids'] \n",
    "    attention_mask = data['attention_mask'] \n",
    "    token_type_ids = data['token_type_ids'] \n",
    "    labels = torch.LongTensor(labels) \n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tmp = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            tmp.append(pred[0])\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def return_img_stream(img_local_path):\n",
    "    img_stream = ''\n",
    "    with open(img_local_path, 'rb') as img_f:\n",
    "        img_stream = img_f.read()\n",
    "        img_stream = base64.b64encode(img_stream).decode()\n",
    "    return img_stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [05/Sep/2022 21:54:04] \"\u001b[37mGET /detail HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Sep/2022 21:54:10] \"\u001b[37mGET /detail HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c86829d166ca73b9ab5589141ffcc20da4f172ad32a8d884566ed63425587a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
